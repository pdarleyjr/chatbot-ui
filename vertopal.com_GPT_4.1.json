{"A new series of GPT models featuring major improvements on coding, instruction following, and long context\u2014plus our first-ever nano model.\nTry in Playground\nListen to article\n18:09\n\nToday, we\u2019re launching three new models in the API: GPT\u20114.1, GPT\u20114.1 mini, and GPT\u20114.1 nano. These models outperform GPT\u20114o and GPT\u20114o mini across the board, with major gains in coding and instruction following. They also have larger context windows\u2014supporting up to 1 million tokens of context\u2014and are able to better use that context with improved long-context comprehension. They feature a refreshed knowledge cutoff of June 2024.\n\nGPT\u20114.1 excels at the following industry standard measures: \n\n    Coding: GPT\u20114.1 scores 54.6% on SWE-bench Verified, improving by 21.4%abs over GPT\u20114o and 26.6%abs over GPT\u20114.5\u2014making it a leading model for coding.\n    Instruction following: On Scale\u2019s MultiChallenge\u2060(opens in a new window) benchmark, a measure of instruction following ability, GPT\u20114.1 scores 38.3%, a 10.5%abs increase over GPT\u20114o.\n    Long context: On Video-MME\u2060(opens in a new window), a benchmark for multimodal long context understanding, GPT\u20114.1 sets a new state-of-the-art result\u2014scoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPT\u20114o.\n\nWhile benchmarks provide valuable insights, we trained these models with a focus on real-world utility. Close collaboration and partnership with the developer community enabled us to optimize these models for the tasks that matter most to their applications.\n\nTo this end, the GPT\u20114.1 model family offers exceptional performance at a lower cost. These models push performance forward at every point on the latency curve.\nGPT-4.1 family intelligence by latency\n\nGPT\u20114.1 mini is a significant leap in small model performance, even beating GPT\u20114o in many benchmarks. It matches or exceeds GPT\u20114o in intelligence evals while reducing latency by nearly half and reducing cost by 83%. \n\nFor tasks that demand low latency, GPT\u20114.1 nano is our fastest and cheapest model available. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding\u2014even higher than GPT\u20114o mini. It\u2019s ideal for tasks like classification or autocompletion.\n\nThese improvements in instruction following reliability and long context comprehension also make the GPT\u20114.1 models considerably more effective at powering agents, or systems that can independently accomplish tasks on behalf of users. When combined with primitives like the Responses API\u2060(opens in a new window), developers can now build agents that are more useful and reliable at real-world software engineering, extracting insights from large documents, resolving customer requests with minimal hand-holding, and other complex tasks. \n\nNote that GPT\u20114.1 will only be available via the API. In ChatGPT, many of the improvements in instruction following, coding, and intelligence have been gradually incorporated into the latest version\u2060(opens in a new window) of GPT\u20114o, and we will continue to incorporate more with future releases. \n\nWe will also begin deprecating GPT\u20114.5 Preview in the API, as GPT\u20114.1 offers improved or similar performance on many key capabilities at much lower cost and latency. GPT\u20114.5 Preview will be turned off in three months, on July 14, 2025, to allow time for developers to transition. GPT\u20114.5 was introduced as a research preview to explore and experiment with a large, compute-intensive model, and we\u2019ve learned a lot from developer feedback. We\u2019ll continue to carry forward the creativity, writing quality, humor, and nuance you told us you appreciate in GPT\u20114.5 into future API models.\n\nBelow, we break down how GPT\u20114.1 performs across several benchmarks, along with examples from alpha testers like Windsurf, Qodo, Hex, Blue J, Thomson Reuters, and Carlyle that showcase how it performs in production on domain-specific tasks.\nCoding\n\nGPT\u20114.1 is significantly better than GPT\u20114o at a variety of coding tasks, including agentically solving coding tasks, frontend coding, making fewer extraneous edits, following diff formats reliably, ensuring consistent tool usage, and more.\n\nOn SWE-bench Verified, a measure of real-world software engineering skills, GPT\u20114.1 completes 54.6% of tasks, compared to 33.2% for GPT\u20114o (2024-11-20). This reflects improvements in model ability to explore a code repository, finish a task, and produce code that both runs and passes tests.\n55%33%41%49%38%24%9%SWE\u2011bench Verified accuracyGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)OpenAI o3-mini (high)GPT-4.5GPT-4.1 miniGPT-4o mini\n\nFor SWE-bench Verified, a model is given a code repository and issue description, and must generate a patch to solve the issue. Performance is highly dependent on the prompts and tools used. To aid in reproducing and contextualizing our results, we describe our setup for GPT\u20114.1 here\u2060(opens in a new window). Our scores omit 23 of 500 problems whose solutions could not run on our infrastructure": null, " if these are conservatively scored as 0, the 54.6% score becomes 52.1%.\n\nFor API developers looking to edit large files, GPT\u20114.1 is much more reliable at code diffs across a range of formats. GPT\u20114.1 more than doubles GPT\u20114o\u2019s score on Aider\u2019s polyglot diff benchmark\u2060(opens in a new window), and even beats GPT\u20114.5 by 8%abs.This evaluation is both a measure of coding capabilities across various programming languages and a measure of model ability to produce changes in whole and diff formats. We\u2019ve specifically trained GPT\u20114.1 to follow diff formats more reliably, which allows developers to save both cost and latency by only having the model output changed lines, rather than rewriting an entire file. For best code diff performance, please refer to our prompting guide\u2060(opens in a new window). For developers who prefer rewriting entire files, we\u2019ve increased output token limits for GPT\u20114.1 to 32,768 tokens (up from 16,384 tokens for GPT\u20114o). We also recommend using Predicted Outputs\u2060(opens in a new window) to reduce latency of full file rewrites.\n52% (whole)53% (diff)31% (whole)18% (diff)64% (whole)62% (diff)67% (whole)60% (diff)35% (whole)32% (diff)10% (whole)6% (diff)4% (whole)3% (diff)N/A (whole)45% (diff)Aider\u2019s polyglot benchmark accuracy\nGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)OpenAI o3-mini (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nIn Aider\u2019s polyglot benchmark, models solve coding exercises from Exercism\u2060(opens in a new window) by editing source files, with one retry allowed. The \u2018whole\u2019 format requires the model to rewrite the entire file, which can be slow and costly. The \u2018diff\u2019 format requires the model to write a series of search/replace blocks\u2060(opens in a new window).\n\nGPT\u20114.1 also substantially improves upon GPT\u20114o in frontend coding, and is capable of creating web apps that are more functional and aesthetically pleasing. In our head-to-head comparisons, paid human graders preferred GPT\u20114.1\u2019s websites over GPT\u20114o\u2019s 80% of the time.\n\nPrompt: Make a flashcard web application. The user should be able to create flashcards, search through their existing flashcards, review flashcards, and see statistics on flashcards reviewed. Preload ten cards containing a Hindi word or phrase and its English translation. Review interface: In the review interface, clicking or pressing Space should flip the card with a smooth 3-D animation to reveal the translation. Pressing the arrow keys should navigate through cards. Search interface: The search bar should dynamically provide a list of results as the user types in a query. Statistics interface: The stats page should show a graph of the number of cards the user has reviewed, and the percentage they have gotten correct. Create cards interface: The create cards page should allow the user to specify the front and back of a flashcard and add to the user's collection. Each of these interfaces should be accessible in the sidebar. Generate a single page React app (put all styles inline).\n\nGPT\u20114o\n\nGPT\u20114.1\n\nBeyond the benchmarks above, GPT\u20114.1 is better at following formats more reliably and makes extraneous edits less frequently. In our internal evals, extraneous edits on code dropped from 9% with GPT\u20114o to 2% with GPT\u20114.1.\nReal world examples\n\nWindsurf\u2060(opens in a new window): GPT\u20114.1 scores 60% higher than GPT\u20114o on Windsurf\u2019s internal coding benchmark, which correlates strongly with how often code changes are accepted on the first review. Their users noted that it was 30% more efficient in tool calling and about 50% less likely to repeat unnecessary edits or read code in overly narrow, incremental steps. These improvements translate into faster iteration and smoother workflows for engineering teams.\n\nQodo\u2060(opens in a new window): Qodo tested GPT\u20114.1 head-to-head against other leading models on generating high-quality code reviews from GitHub pull requests using a methodology inspired by their fine-tuning benchmark. Across 200 meaningful real-world pull requests with the same prompts and conditions, they found that GPT\u20114.1 produced the better suggestion in 55% of cases\u2060(opens in a new window). Notably, they found that GPT\u20114.1 excels at both precision (knowing when not to make suggestions) and comprehensiveness (providing thorough analysis when warranted), while maintaining focus on truly critical issues.\nInstruction following\n\nGPT\u20114.1 follows instructions more reliably, and we\u2019ve measured significant improvements across a variety of instruction following evals.\n\nWe developed an internal eval for instruction following to track model performance across a number of dimensions and in several key categories of instruction following, including:\n\n    Format following. Providing instructions that specify a custom format for the model\u2019s response, such as XML, YAML, Markdown, etc.\n    Negative instructions. Specifying behavior the model should avoid. (Example: \u201cDon\u2019t ask the user to contact support\u201d)\n    Ordered instructions. Providing a set of instructions the model must follow in a given order. (Example: \u201cFirst ask for the user's name, then ask for their email\u201d)\n    Content requirements. Outputting content that includes certain information. (Example: \u201cAlways include amount of protein when writing a nutrition plan\u201d)\n    Ranking. Ordering the output in a particular way. (Example: \u201cSort the response by population count\u201d)\n    Overconfidence. Instructing the model to say \u201cI don't know\u201d or similar if requested information isn't available, or the request doesn\u2019t fall in a given category. (Example: \u201cIf you do not know the answer, provide the support contact email\u201d)\n\nThese categories are the result of feedback from developers regarding which facets of instruction following are most relevant and important to them. Within each category, we\u2019ve split up easy, medium, and hard prompts. GPT\u20114.1 improves significantly over GPT\u20114o on hard prompts in particular.\n49%29%51%50%54%45%32%27%Internal OpenAI Instructions following eval accuracy (hard subset)GPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)OpenAI o3-mini (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nOur internal instruction following eval is based on real developer use cases and feedback, covering tasks of varying complexity coupled with instructions on formatting, verbosity, length, and more.\n\nMulti-turn instruction following is critical for many developers\u2014it\u2019s important for the model to maintain coherence deep into a conversation, and keep track of what the user told it earlier. We\u2019ve trained GPT\u20114.1 to be better able to pick out information from past messages in the conversation, allowing for more natural conversations. The MultiChallenge benchmark from Scale is a useful measure of this capability, and GPT\u20114.1 performs 10.5%abs better than GPT\u20114o.\n38%28%45%40%44%36%15%20%MultiChallenge accuracyGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)OpenAI o3-mini (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nIn MultiChallenge\u2060(opens in a new window), models are challenged on multi-turn conversations to properly use four types of information from previous messages.\n\nGPT\u20114.1 also scores 87.4% on IFEval, compared to 81.0% for GPT\u20114o. IFEval uses prompts with verifiable instructions (for example, specifying content length or avoiding certain terms or formats).\n87%81%92%94%88%84%75%78%IFEval accuracyGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)OpenAI o3-mini (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nIn IFEval\u2060(opens in a new window), models must generate answers that comply with various instructions.\n\nBetter instruction following makes existing applications more reliable, and enables new applications previously limited by poor reliability. Early testers noted that GPT\u20114.1 can be more literal, so we recommend being explicit and specific in prompts. For more on prompting best practices for GPT\u20114.1, please refer to the prompting guide.\nReal world examples\n\nBlue J\u2060(opens in a new window): GPT\u20114.1 was 53% more accurate than GPT\u20114o on an internal benchmark of Blue J\u2019s most challenging real-world tax scenarios. This jump in accuracy\u2014key to both system performance and user satisfaction\u2014highlights GPT\u20114.1\u2019s improved comprehension of complex regulations and its ability to follow nuanced instructions over long contexts. For Blue J users, that means faster, more reliable tax research and more time for high-value advisory work.\n\nHex\u2060(opens in a new window): GPT\u20114.1 delivered a nearly 2\u00d7 improvement on Hex\u2019s most challenging SQL evaluation set,\u2060(opens in a new window) showcasing significant gains in instruction following and semantic understanding. The model was more reliable in selecting the correct tables from large, ambiguous schemas\u2014an upstream decision point that directly impacts overall accuracy and is difficult to tune through prompting alone. For Hex, this resulted in a measurable reduction in manual debugging and a faster path to production-grade workflows.\nLong Context\n\nGPT\u20114.1, GPT\u20114.1 mini, and GPT\u20114.1 nano can process up to 1 million tokens of context\u2014up from 128,000 for previous GPT\u20114o models. 1 million tokens is more than 8 copies of the entire React codebase, so long context is a great fit for processing large codebases, or lots of long documents.\n\nWe trained GPT\u20114.1 to reliably attend to information across the full 1 million context length. We\u2019ve also trained it to be far more reliable than GPT\u20114o at noticing relevant text, and ignoring distractors across long and short context lengths. Long-context understanding is a critical capability for applications across legal, coding, customer support, and many other domains.\n\nBelow, we demonstrate GPT\u20114.1\u2019s ability to retrieve a small hidden piece of information (a \u201cneedle\u201d) positioned at various points within the context window. GPT\u20114.1 consistently retrieves the needle accurately at all positions and all context lengths, all the way up to 1 million tokens. It is effectively able to pull out relevant details for the task at hand regardless of their position in the input.\nGPT-4.1 Needle in a Haystack accuracy graph\n\nIn our internal needle in a haystack eval, GPT\u20114.1, GPT\u20114.1 mini, and GPT 4.1 nano are all able to retrieve the needle at all positions in the context up to 1M.\n\nHowever, few real-world tasks are as straightforward as retrieving a single, obvious needle answer. We find users often need our models to retrieve and understand multiple pieces of information, and to understand those pieces in relation to each other. To showcase this capability, we\u2019re open-sourcing a new eval: OpenAI-MRCR (Multi-Round Coreference). \n\nOpenAI-MRCR tests the model\u2019s ability to find and disambiguate between multiple needles well hidden in context. The evaluation consists of multi-turn synthetic conversations between a user and assistant where the user asks for a piece of writing about a topic, for example \"write a poem about tapirs\" or \"write a blog post about rocks\". We then insert two, four, or eight identical requests throughout the context. The model must then retrieve the response corresponding to a specific instance (e.g., \u201cgive me the third poem about tapirs\u201d).\n\nThe challenge arises from the similarity between these requests and the rest of the context\u2014models can easily be misled by subtle differences, such as a short story about tapirs rather than a poem, or a poem about frogs instead of tapirs. We find that GPT\u20114.1 outperforms GPT\u20114o at context lengths up to 128K tokens and maintains strong performance even up to 1 million tokens.\n\nBut the task remains hard\u2014even for advanced reasoning models. We\u2019re sharing the eval dataset\u2060(opens in a new window) to encourage further work on real-world long-context retrieval.\nOpenAI MRCR accuracy, 2 needle\n\nIn OpenAI-MRCR\u2060(opens in a new window), the model must answer a question that involves disambiguating between 2, 4, or 8 user prompts scattered amongst distractors.\nOpenAI MRCR accuracy, 4 needle\n\nIn OpenAI-MRCR\u2060(opens in a new window), the model must answer a question that involves disambiguating between 2, 4, or 8 user prompts scattered amongst distractors.\nOpenAI MRCR accuracy, 8 needle\n\nIn OpenAI-MRCR\u2060(opens in a new window), the model must answer a question that involves disambiguating between 2, 4, or 8 user prompts scattered amongst distractors.\n\nWe\u2019re also releasing Graphwalks\u2060(opens in a new window), a dataset for evaluating multi-hop long-context reasoning. Many developer use cases for long context require multiple logical hops within the context, like jumping between multiple files when writing code or cross referencing documents when answering complicated legal questions.\n\nA model (or even a human) could theoretically solve an OpenAI-MRCR problem by doing one pass or read-through of the prompt, but Graphwalks is designed to require reasoning across multiple positions in the context and cannot be solved sequentially.\n\nGraphwalks fills the context window with a directed graph composed of hexadecimal hashes, and then asks the model to perform a breadth-first search (BFS) starting from a random node in the graph. We then ask it to return all nodes at a certain depth. GPT\u20114.1 achieves 61.7% accuracy on this benchmark, matching the performance of o1 and beating GPT\u20114o handily.\n62%42%62%51%72%62%25%29%Graphwalks BFS <128k accuracyGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)OpenAI o3-mini (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nIn Graphwalks\u2060(opens in a new window), a model is asked to perform a breadth-first search from a random node in a large graph.\n\nBenchmarks don\u2019t tell the full story, so we worked with alpha partners to test the performance of GPT\u20114.1 on their real-world long context tasks.\nReal world examples\n\nThomson Reuters:\u2060(opens in a new window) Thomson Reuters tested GPT\u20114.1 with CoCounsel, their professional grade AI assistant for legal work. Compared to GPT\u20114o, they were able to improve multi-document review accuracy by 17% when using GPT\u20114.1 across internal long-context benchmarks\u2014an essential measure of CoCounsel\u2019s ability to handle complex legal workflows involving multiple, lengthy documents. In particular, they found the model to be highly reliable at maintaining context across sources and accurately identifying nuanced relationships between documents, such as conflicting clauses or additional supplementary context\u2014tasks critical to legal analysis and decision-making.\n\nCarlyle\u2060(opens in a new window): Carlyle used GPT\u20114.1 to accurately extract granular financial data across multiple, lengthy documents\u2014including PDFs, Excel files, and other complex formats. Based on their internal evaluations, it performed 50% better on retrieval from very large documents with dense data and was the first model to successfully overcome key limitations seen with other available models, including needle-in-the-haystack retrieval, lost-in-the-middle errors, and multi-hop reasoning across documents.\n\nIn addition to model performance and accuracy, developers also need models that respond quickly to keep up with and meet users\u2019 needs. We've improved our inference stack to reduce the time to first token, and with prompt caching, you can cut latency even further while saving on costs. In our initial testing, latency to first token for GPT\u20114.1 was approximately fifteen seconds with 128,000 tokens of context, and a minute for a million tokens of context. GPT\u20114.1 mini and nano are faster, e.g., GPT\u20114.1 nano most often returns the first token in less than five seconds for queries with 128,000 input tokens.\nVision\n\nThe GPT\u20114.1 family is exceptionally strong at image understanding, with GPT\u20114.1 mini in particular representing a significant leap forward, often beating GPT\u20114o on image benchmarks.\n75%69%78%75%73%55%56%MMMU accuracyGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nIn MMMU\u2060(opens in a new window), a model answers questions containing charts, diagrams, maps, etc. (Note: even when the image is not included, many answers can still be inferred or guessed from context.)\n72%61%72%72%73%56%57%MathVista accuracyGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nIn MathVista\u2060(opens in a new window), a model solves visual mathematical tasks.\n57%53%55%55%57%41%37%CharXiv-Reasoning accuracyGPT-4.1GPT-4o (2024-11-20)OpenAI o1 (high)GPT-4.5GPT-4.1 miniGPT-4.1 nanoGPT-4o mini\n\nIn CharXiv-Reasoning\u2060(opens in a new window), a model answers questions about charts from scientific papers.\n\nLong context performance is also important for multimodal use cases, such as processing long videos. In Video-MME\u2060(opens in a new window) (long w/o subs), a model answers multiple choice questions based on 30-60 minute long videos with no subtitles. GPT\u20114.1 achieves state-of-the-art performance, scoring 72.0%, up from 65.3% for GPT\u20114o.\n72%65%Video long contextGPT-4.1GPT-4o (2024-11-20)\n\nIn Video-MME\u2060(opens in a new window), a model answers multiple choice questions based on 30-60 minute long videos with no subtitles.\nPricing\n\nGPT\u20114.1, GPT\u20114.1 mini, and GPT\u20114.1 nano are available now to all developers. \n\nThrough efficiency improvements to our inference systems, we\u2019ve been able to offer lower prices on the GPT\u20114.1 series.GPT\u20114.1 is 26% less expensive than GPT\u20114o for median queries, and GPT\u20114.1 nano is our cheapest and fastest model ever. For queries that repeatedly pass the same context, we are increasing the prompt caching discount to 75% (up from 50% previously) for these new models. Finally, we offer long context requests at no additional cost beyond the standard per-token costs.\n\nModel\n(Prices are per 1M tokens)\n\t\n\nInput\n\t\n\nCached input\n\t\n\nOutput\n\t\n\nBlended Pricing*\n\ngpt-4.1\n\t\n\n$2.00\n\t\n\n$0.50\n\t\n\n$8.00\n\t\n\n$1.84\n\ngpt-4.1-mini\n\t\n\n$0.40\n\t\n\n$0.10\n\t\n\n$1.60\n\t\n\n$0.42\n\ngpt-4.1-nano\n\t\n\n$0.10\n\t\n\n$0.025\n\t\n\n$0.40\n\t\n\n$0.12\n\n*Based on typical input/output and cache ratios.\n\nThese models are available for use in our Batch API\u2060(opens in a new window) at an additional 50% pricing discount.\nConclusion\n\nGPT\u20114.1 is a significant step forward in the practical application of AI. By focusing closely on real-world developer needs\u2014ranging from coding to instruction-following and long context understanding\u2014these models unlock new possibilities for building intelligent systems and sophisticated agentic applications. We\u2019re continually inspired by the developer community\u2019s creativity, and are excited to see what you build with GPT\u20114.1.\nAppendix\n\nA full list of results across academic, coding, instruction following, long context, vision, and function calling evals can be found below.\nAcademic knowledge\nCategory\tGPT-4.1\tGPT-4.1 mini\tGPT-4.1 nano\tGPT-4o (2024-11-20)\tGPT-4o mini\tOpenAI o1 (high)\tOpenAI o3-mini (high)\tGPT-4.5\nAIME '24\t48.1%\t49.6%\t29.4%\t13.1%\t8.6%\t74.3%\t87.3%\t36.7%\nGPQA Diamond1\t66.3%\t65.0%\t50.3%\t46.0%\t40.2%\t75.7%\t77.2%\t69.5%\nMMLU\t90.2%\t87.5%\t80.1%\t85.7%\t82.0%\t91.8%\t86.9%\t90.8%\nMultilingual MMLU\t87.3%\t78.5%\t66.9%\t81.4%\t70.5%\t87.7%\t80.7%\t85.1%\n\n[1] Our implementation of GPQA uses a model to extract the answer instead of regex. For GPT-4.1, the difference was <1% (not statistically significant), but for GPT-4o model extraction improves scores significantly (~46% -> 54%).\nCoding evals\nCategory\tGPT-4.1\tGPT-4.1 mini\tGPT-4.1 nano\tGPT-4o (2024-11-20)\tGPT-4o mini\tOpenAI o1 (high)\tOpenAI o3-mini (high)\tGPT-4.5\nSWE-bench Verified2\t54.6%\t23.6%\t-\t33.2%\t8.7%\t41.0%\t49.3%\t38.0%\nSWE-Lancer\t$176K\n(35.1%)\t$165K\n(33.0%)\t$77K\n(15.3%)\t$163K\n(32.6%)\t$116K\n(23.1%)\t$160K\n(32.1%)\t$90K\n(18.0%)\t$186K\n(37.3%)\nSWE-Lancer (IC-Diamond subset)\t$34K\n(14.4%)\t$31K\n(13.1%)\t$9K\n(3.7%)\t$29K\n(12.4%)\t$11K\n(4.8%)\t$29K\n(9.7%)\t$17K\n(7.4%)\t$41K\n(17.4%)\nAider\u2019s polyglot: whole\t51.6%\t34.7%\t9.8%\t30.7%\t3.6%\t64.6%\t66.7%\t-\nAider\u2019s polyglot: diff\t52.9%\t31.6%\t6.2%\t18.2%\t2.7%\t61.7%\t60.4%\t44.9%\n\n[2] We omit 23/500 problems that could not run on our infrastructure. The full list of 23 tasks omitted are 'astropy__astropy-7606', 'astropy__astropy-8707', 'astropy__astropy-8872', 'django__django-10097', 'django__django-7530', 'matplotlib__matplotlib-20488', 'matplotlib__matplotlib-20676', 'matplotlib__matplotlib-20826', 'matplotlib__matplotlib-23299', 'matplotlib__matplotlib-24970', 'matplotlib__matplotlib-25479', 'matplotlib__matplotlib-26342', 'psf__requests-6028', 'pylint-dev__pylint-6528', 'pylint-dev__pylint-7080', 'pylint-dev__pylint-7277', 'pytest-dev__pytest-5262', 'pytest-dev__pytest-7521', 'scikit-learn__scikit-learn-12973', 'sphinx-doc__sphinx-10466', 'sphinx-doc__sphinx-7462', 'sphinx-doc__sphinx-8265', and 'sphinx-doc__sphinx-9367'.\nInstruction following\nCategory\tGPT-4.1\tGPT-4.1 mini\tGPT-4.1 nano\tGPT-4o (2024-11-20)\tGPT-4o mini\tOpenAI o1 (high)\tOpenAI o3-mini (high)\tGPT-4.5\nInternal API instruction following (hard)\t49.1%\t45.1%\t31.6%\t29.2%\t27.2%\t51.3%\t50.0%\t54.0%\nMultiChallenge\t38.3%\t35.8%\t15.0%\t27.8%\t20.3%\t44.9%\t39.9%\t43.8%\nMultiChallenge (o3-mini grader)3\t46.2%\t42.2%\t31.1%\t39.9%\t25.6%\t52.9%\t50.2%\t50.1%\nCOLLIE\t65.8%\t54.6%\t42.5%\t50.2%\t52.7%\t95.3%\t98.7%\t72.3%\nIFEval\t87.4%\t84.1%\t74.5%\t81.0%\t78.4%\t92.2%\t93.9%\t88.2%\nMulti-IF\t70.8%\t67.0%\t57.2%\t60.9%\t57.9%\t77.9%\t79.5%\t70.8%\n\n[3] Note: we find that the default grader in MultiChallenge (GPT-4o) frequently mis-scores model responses. We find that swapping the grader to a reasoning model, like o3-mini, improves accuracy on grading significantly on samples we\u2019ve inspected. For consistency reasons with the leaderboard, we\u2019re publishing both sets of results..Note: we find that the default grader in MultiChallenge (GPT-4o) frequently mis-scores model responses. We find that swapping the grader to a reasoning model, like o3-mini, improves accuracy on grading significantly on samples we\u2019ve inspected. For consistency reasons with the leaderboard, we\u2019re publishing both sets of results.\nLong context evals\nCategory\tGPT-4.1\tGPT-4.1 mini\tGPT-4.1 nano\tGPT-4o (2024-11-20)\tGPT-4o mini\tOpenAI o1 (high)\tOpenAI o3-mini (high)\tGPT-4.5\nOpenAI-MRCR: 2 needle128k\t57.2%\t47.2%\t36.6%\t31.9%\t24.5%\t22.1%\t18.7%\t38.5%\nOpenAI-MRCR: 2 needle 1M\t46.3%\t33.3%\t12.0%\t-\t-\t-\t-\t-\nGraphwalks bfs <128k\t61.7%\t61.7%\t25.0%\t41.7%\t29.0%\t62.0%\t51.0%\t72.3%\nGraphwalks bfs >128k\t19.0%\t15.0%\t2.9%\t-\t-\t-\t-\t-\nGraphwalks parents <128k\t58.0%\t60.5%\t9.4%\t35.4%\t12.6%\t50.9%\t58.3%\t72.6%\nGraphwalks parents >128k\t25.0%\t11.0%\t5.6%\t-\t-\t-\t-\t-\nVision\nCategory\tGPT-4.1\tGPT-4.1 mini\tGPT-4.1 nano\tGPT-4o (2024-11-20)\tGPT-4o mini\tOpenAI o1 (high)\tOpenAI o3-mini (high)\tGPT-4.5\nMMMU\t74.8%\t72.7%\t55.4%\t68.7%\t56.3%\t77.6%\t-\t75.2%\nMathVista\t72.2%\t73.1%\t56.2%\t61.4%\t56.5%\t71.8%\t-\t72.3%\nCharXiv-R\t56.7%\t56.8%\t40.5%\t52.7%\t36.8%\t55.1%\t-\t55.4%\nCharXiv-D\t87.9%\t88.4%\t73.9%\t85.3%\t76.6%\t88.9%\t-\t90.0%\nFunction calling\nCategory\tGPT-4.1\tGPT-4.1 mini\tGPT-4.1 nano\tGPT-4o (2024-11-20)\tGPT-4o mini\tOpenAI o1 (high)\tOpenAI o3-mini (high)\tGPT-4.5\nComplexFuncBench\t65.5%\t49.3%\t5.7%\t66.5%\t38.6%\t47.6%\t17.6%\t63.0%\nTaubench airline4\t49.4%\t36.0%\t14.0%\t42.8%\t22.0%\t50.0%\t32.4%\t50.0%\nTaubench retail4, 5\t68.0%\n(73.6%)\t55.8%\n(65.4%)\t22.6%\n(23.5%)\t60.3%\t44.0%\t70.8%\t57.6%\t68.4%\n\n[4] tau-bench eval numbers are averaged across 5 runs to reduce variance, and run without any custom tools or prompting.\n\n[5] Numbers in parentheses represent Tau-bench results when using GPT-4.1 as the user model, rather than GPT-4o. We\u2019ve found that, since GPT-4.1 is better at instruction following, it is better able to perform as the user, and so results in more successful trajectories. We believe this represents the true performance of the evaluated model on the benchmark.": null}